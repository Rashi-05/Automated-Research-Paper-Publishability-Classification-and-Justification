{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8CYlHzfaJXY3"
   },
   "source": [
    "# <u>Automated Research Paper Publishability Classification and Justification<u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "3X7zc5J-MwN3",
    "outputId": "4c13e8c6-5fab-481d-d1b0-9bfd985aac6c"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QfZ2FCmSM96a",
    "outputId": "e7225181-af92-40d2-a921-d337a2d1f8c8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RMKb5-vfTtgw",
    "outputId": "00d6249a-a8df-488f-84f6-a8e33c839b48"
   },
   "outputs": [],
   "source": [
    "pip install fitz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "C3qxRGl9Uv0y",
    "outputId": "b0285c1a-3660-41e6-a548-b2b5b31c2277"
   },
   "outputs": [],
   "source": [
    "pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ccTcWE8ZZd0",
    "outputId": "f374cd0e-aa64-4d5b-a701-7a9f9affe7f8"
   },
   "outputs": [],
   "source": [
    "pip install PyMuPDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "itqOf5N26ujl"
   },
   "outputs": [],
   "source": [
    "!pip install transformers --quiet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bPPA5n5WUVRs",
    "outputId": "dfd5fb91-6829-4299-b3a6-704f0b613e56"
   },
   "outputs": [],
   "source": [
    "pip install tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1E9lzAAmTuSV"
   },
   "outputs": [],
   "source": [
    "import fitz  # for PDF text extraction (PyMuPDF)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PyPDF2 import PdfReader # For pdf extraction\n",
    "from sentence_transformers import SentenceTransformer # for embeddings\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score,confusion_matrix\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, fbeta_score\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "tIDeEGnUYNGJ",
    "outputId": "6175bd4f-e6c3-4d25-ddf5-5698ef46a465"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPzBKQJFX5ph"
   },
   "source": [
    "# Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "id": "Gh39C9qEVZkg",
    "outputId": "a831bb96-08c4-4411-95c8-65e24d1c1f1e"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W-cl154GVZry",
    "outputId": "df0382d4-df1f-4b95-dfd6-311f5f144b03"
   },
   "outputs": [],
   "source": [
    "# Get the uploaded filename\n",
    "zip_filename = next(iter(uploaded))\n",
    "\n",
    "# Create a folder to extract to\n",
    "extract_to = \"EARC_Dataset\"\n",
    "os.makedirs(extract_to, exist_ok=True)\n",
    "\n",
    "# Extracting\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extract_to)\n",
    "\n",
    "print(\"Extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nlKimY2YV4UC",
    "outputId": "bf8a7973-d440-472d-f9b4-d13bbfe0ce15"
   },
   "outputs": [],
   "source": [
    "# Folder overview\n",
    "for root, dirs, files in os.walk(\"EARC_Dataset\"):\n",
    "    print(\"Directory:\", root)\n",
    "    print(\"Subdirectories:\", dirs)\n",
    "    print(\"Files:\", files)\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VxUP-w3dV4Wj"
   },
   "outputs": [],
   "source": [
    "# Base directory after extraction\n",
    "base_dir = \"EARC_Dataset/EARC Dataset\"\n",
    "\n",
    "# Reference folder (contains both Publishable and Non-Publishable)\n",
    "reference_dir = os.path.join(base_dir, \"Reference\")\n",
    "\n",
    "# Papers folder\n",
    "papers_dir = os.path.join(base_dir, \"Papers\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "k6z2BTonYLl3",
    "outputId": "b6df14c0-e887-4dd9-82d7-f6b5bc42ae6e"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BAkPJUeAYFET"
   },
   "source": [
    "# Loading and Structuring the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "id": "Je2Ym2IHX4w1",
    "outputId": "72f92978-e9bb-4e19-8a71-dda80ea93687"
   },
   "outputs": [],
   "source": [
    "# Loading and structuring the labelled structured data\n",
    "\n",
    "# Initializing the list to collect data\n",
    "data = []\n",
    "\n",
    "# Loop through both label folders: Publishable and Non-Publishable\n",
    "for label in [\"Publishable\", \"Non-Publishable\"]:\n",
    "    label_dir = os.path.join(reference_dir, label)\n",
    "\n",
    "    if not os.path.exists(label_dir):\n",
    "        print(f\"⚠ Warning: Path not found: {label_dir}\")\n",
    "        continue\n",
    "\n",
    "    for file in os.listdir(label_dir):\n",
    "        if file.endswith(\".pdf\"):\n",
    "            file_path = os.path.join(label_dir, file)\n",
    "\n",
    "            try:\n",
    "                reader = PdfReader(file_path)  # Using PdfReader to load the pdf\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text() or \"\"\n",
    "\n",
    "                data.append({\n",
    "                    \"Filename\": file,\n",
    "                    \"Text\": text,\n",
    "                    \"Label\": 1 if label == \"Publishable\" else 0\n",
    "                })\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file_path}: {e}\")\n",
    "\n",
    "# Converting list to a DataFrame\n",
    "df_labeled= pd.DataFrame(data)\n",
    "print(\"Labeled training data shape:\", df_labeled.shape)\n",
    "df_labeled                                   # 1 stands for publishable and 0 for non-publishable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 539
    },
    "id": "hgrsUvf7bOjr",
    "outputId": "fe674f1d-aad0-4039-9d38-2eb1ec9d561c"
   },
   "outputs": [],
   "source": [
    "# Checking target column distribution\n",
    "\n",
    "df_labeled[\"Label\"].value_counts(normalize=True).plot.pie(autopct='%1.1f%%', labels=['Publishable', 'Non-Publishable'], figsize=(6, 6), title=\"Label Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 242
    },
    "id": "Qc8-l8AcYhdA",
    "outputId": "d2a15bb8-79ce-439f-8838-81c0f062c968"
   },
   "outputs": [],
   "source": [
    "# Createing dataframe for unlabelled papers\n",
    "\n",
    "# Path to all papers folder (the 150 papers)\n",
    "papers_path = os.path.join(base_dir, \"Papers\")\n",
    "\n",
    "# Get list of all filenames in papers folder\n",
    "all_papers_files = os.listdir(papers_path)\n",
    "\n",
    "# Getting list of labeled filenames (from your labeled DataFrame 'df')\n",
    "labeled_files =df_labeled['Filename'].tolist()\n",
    "\n",
    "# Filtering out labeled files to get only unlabeled paper filenames\n",
    "unlabeled_files = [f for f in all_papers_files if f not in labeled_files]\n",
    "\n",
    "print(f\"Number of unlabeled papers: {len(unlabeled_files)}\")  # Should come 135\n",
    "\n",
    "# Function to extract text from pdf\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    try:\n",
    "        with fitz.open(pdf_path) as doc:\n",
    "            for page in doc:\n",
    "                text += page.get_text()\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {pdf_path}: {e}\")\n",
    "    return text\n",
    "\n",
    "unlabeled_data = []\n",
    "\n",
    "# Extracting text for each unlabeled paper\n",
    "for filename in unlabeled_files:\n",
    "    full_path = os.path.join(papers_path, filename)\n",
    "    text = extract_text_from_pdf(full_path)\n",
    "\n",
    "    # Skipping if text is empty or too short (filter noisy files)\n",
    "    if len(text.strip()) < 100:\n",
    "        print(f\"Skipping {filename} due to insufficient text\")\n",
    "        continue\n",
    "\n",
    "    unlabeled_data.append({\n",
    "        \"Filename\": filename,\n",
    "        \"Text\": text,\n",
    "        \"Label\": None  # no label for unlabeled data\n",
    "    })\n",
    "\n",
    "df_unlabeled = pd.DataFrame(unlabeled_data)\n",
    "\n",
    "print(f\"Unlabeled DataFrame shape: {df_unlabeled.shape}\")\n",
    "df_unlabeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "TzhH8z3uV4Zs",
    "outputId": "a64571cf-a975-4e67-cf3d-451785539025"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKkIutOgae9w"
   },
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FryoxsBIj8Db"
   },
   "source": [
    "## Cleaning Labelled and Unlabelled Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NmtkWe4QaeBI"
   },
   "outputs": [],
   "source": [
    "# Cleaning of both labelled and unlabelled data\n",
    "\n",
    "def clean_text(text):\n",
    "    # Removing references section (if present)\n",
    "    text = re.split(r'\\bReferences\\b|\\bREFERENCES\\b', text)[0]\n",
    "    # Removing non-ASCII characters\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+', ' ', text)\n",
    "    # Removing extra whitespace and newlines\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Lowercasing\n",
    "    text = text.lower()\n",
    "    # Removing punctuation\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    text = re.sub(r'\\[[0-9]+\\]', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9.,;:?! ]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Strip whitespace\n",
    "    return text.strip()\n",
    "\n",
    "def filter_short_text(text, min_words=100):\n",
    "    return len(text.split()) >= min_words\n",
    "\n",
    "\n",
    "# Appling to labeled data\n",
    "df_labeled['cleaned_text'] = df_labeled['Text'].apply(clean_text)\n",
    "\n",
    "# Appling to unlabeled data\n",
    "df_unlabeled['cleaned_text'] = df_unlabeled['Text'].apply(clean_text)\n",
    "\n",
    "df_labeled = df_labeled[df_labeled['Text'].apply(filter_short_text)].reset_index(drop=True)\n",
    "df_unlabeled = df_unlabeled[df_unlabeled['Text'].apply(filter_short_text)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 143
    },
    "id": "BmUbsDmvcGWF",
    "outputId": "5b996fb1-e800-49d8-ef56-332ac4ea90eb"
   },
   "outputs": [],
   "source": [
    "df_labeled.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "bBR4UyfL3__5",
    "outputId": "e5b33542-19cd-4738-ec4c-de4cf860a96d"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yNZ4doS8kF3z"
   },
   "source": [
    "## Checking Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "195MRw8HdCGx",
    "outputId": "7cfc36dd-9e9d-49a5-d759-7ca3c7271725"
   },
   "outputs": [],
   "source": [
    "# For labeled data\n",
    "print(\"Missing values in df_labeled:\")\n",
    "print(df_labeled.isnull().sum().sum())\n",
    "\n",
    "# For unlabeled data\n",
    "print(\"\\nMissing values in df_unlabeled:\")\n",
    "print(df_unlabeled.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "V65S_Mpo43JU",
    "outputId": "b5b35bb5-b993-4003-9b54-05cc4ad521fa"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "movDuxfyYAGu"
   },
   "source": [
    "## Section Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4urs9OTBV4j5"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text):\n",
    "    sections = {\n",
    "        'abstract': '',\n",
    "        'introduction': '',\n",
    "        'methodology': '',\n",
    "        'results': '',\n",
    "        'conclusion': ''\n",
    "    }\n",
    "\n",
    "    # Normalizing text\n",
    "    text = text.replace('\\n', ' ').replace('\\r', ' ')\n",
    "    text_lower = text.lower()\n",
    "\n",
    "    # Defining patterns for headings\n",
    "    patterns = {\n",
    "        'abstract': r'\\babstract\\b',\n",
    "        'introduction':r'\\b(introduction|background)\\b',\n",
    "        'methodology': r'\\bmethodology\\b|\\bmaterials and methods\\b|\\bmethods\\b|\\bapproach\\b',\n",
    "        'results': r'\\bresults\\b|\\bresults and discussion\\b',\n",
    "        'conclusion': r'\\bconclusion\\b|\\bconclusions\\b'\n",
    "    }\n",
    "\n",
    "    # Finding the positions of each heading\n",
    "    positions = {}\n",
    "    for section, pattern in patterns.items():\n",
    "        match = re.search(pattern, text_lower)\n",
    "        if match:\n",
    "            positions[section] = match.start()\n",
    "        else:\n",
    "            positions[section] = -1\n",
    "\n",
    "    # Defining ordered section list\n",
    "    section_order = ['abstract', 'introduction', 'methodology', 'results', 'conclusion']\n",
    "\n",
    "    # Extracting sections based on position differences\n",
    "    for i in range(len(section_order)):\n",
    "        start_sec = section_order[i]\n",
    "        start_pos = positions[start_sec]\n",
    "\n",
    "        if start_pos == -1:\n",
    "            continue\n",
    "\n",
    "        # Finding next available section to determine end of current\n",
    "        end_pos = None\n",
    "        for j in range(i+1, len(section_order)):\n",
    "            next_sec = section_order[j]\n",
    "            if positions[next_sec] != -1:\n",
    "                end_pos = positions[next_sec]\n",
    "                break\n",
    "\n",
    "        if end_pos:\n",
    "            sections[start_sec] = text[start_pos:end_pos].strip()\n",
    "        else:\n",
    "            sections[start_sec] = text[start_pos:].strip()\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMzPJVhyVKDR"
   },
   "outputs": [],
   "source": [
    "def add_sections(row):\n",
    "    secs = extract_sections(row['Text'])\n",
    "    for k, v in secs.items():\n",
    "        row[k] = v if v else \"[No content]\"\n",
    "    return row\n",
    "\n",
    "df_labeled = df_labeled.apply(add_sections, axis=1)\n",
    "df_unlabeled = df_unlabeled.apply(add_sections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "7RilSfdWd-uf",
    "outputId": "78890f92-ff00-482b-df90-dcccad8bcbb1"
   },
   "outputs": [],
   "source": [
    "df_labeled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "ZbMGEdjj5dTS",
    "outputId": "13eb21df-880a-44a7-9d2b-bf13a068f9f0"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z6i9qRn9eYFC"
   },
   "source": [
    "## Handling Empty or Missing Sections\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rj-pO8mpktCv"
   },
   "outputs": [],
   "source": [
    "def is_missing(text):\n",
    "    if not isinstance(text, str):\n",
    "        return True\n",
    "    text = text.strip().lower()\n",
    "    return text in ['', '[no content]', 'n/a', 'none', 'na']  # Robust check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5KbjMyB4i8la",
    "outputId": "2960b4f8-9386-408b-d221-3e8b012a58c2"
   },
   "outputs": [],
   "source": [
    "section_cols = ['abstract', 'introduction', 'methodology', 'results', 'conclusion']\n",
    "\n",
    "for col in section_cols:\n",
    "    print(f\"{col} — Missing or placeholder:\", df_labeled[col].apply(is_missing).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y5bLWhXpfCHU"
   },
   "source": [
    "Handling missing/empty sections using a fallback strategy, in which if a section is missing or too short, it is replaced with the most similar available section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKTJIk85izCM"
   },
   "outputs": [],
   "source": [
    "def fill_missing_sections(row):\n",
    "    if is_missing(row['abstract']):\n",
    "        row['abstract'] = row['Text'][:400]  # Fallback: Use first 400 chars of full text\n",
    "    if is_missing(row['introduction']):\n",
    "        row['introduction'] = row['abstract']  # Use abstract as fallback\n",
    "    if is_missing(row['methodology']):\n",
    "        row['methodology'] = row['introduction']\n",
    "    if is_missing(row['results']):\n",
    "        row['results'] = row['conclusion']\n",
    "    if is_missing(row['conclusion']):\n",
    "        row['conclusion'] = row['results']\n",
    "    return row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tdaAF_75ecLV"
   },
   "outputs": [],
   "source": [
    "df_labeled = df_labeled.apply(fill_missing_sections, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G1LecTxleZqC",
    "outputId": "f5bc85e6-dfd5-4e8d-9ede-f368e942a5aa"
   },
   "outputs": [],
   "source": [
    "# Cross checking\n",
    "for col in section_cols:\n",
    "    print(f\"{col} — Missing or placeholder:\", df_labeled[col].apply(is_missing).sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JBvQgpuOhgFW"
   },
   "source": [
    "There are no missing sections in labelled dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "kWKXhOEKeiyl",
    "outputId": "4d4a6e02-6964-482e-d9a1-dc6b12585c77"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W57jIx75m9fB"
   },
   "source": [
    "## <u>Text Vectorisation using Transformer Models<u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWXXy41jm9MH"
   },
   "source": [
    "Research papers are long texts. Using traditional text representations like Bag-of-Words or TF-IDF lose semantic meaning and context.Therefore we will use transformer based embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEjGboRwnHC7"
   },
   "source": [
    "Instead of combining all sections into one big blob (which loses section-specific meaning), the best practice is:\n",
    "\n",
    "Generate separate embeddings per section → then concatenate them into a single vector per paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WlohoFWyk6lP"
   },
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('allenai-specter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TpT60UPrnwr7"
   },
   "outputs": [],
   "source": [
    "def get_norm_embedding(text):\n",
    "    emb = model.encode(text)\n",
    "    return normalize(emb.reshape(1, -1)).flatten()\n",
    "\n",
    "df_labeled['embedding'] = df_labeled['Text'].apply(get_norm_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "281401c2e6474714ad1cc7c4542bdd7e",
      "a6a9960e447e4c88a035a0dd64ed8f60",
      "c597814dd68041acbc398c3268d214b4",
      "bece013b8c1e4e9c9bd8b2d24e545341",
      "0b0ffa701df1406a8c3d0926c02c627a",
      "ccdeb5d2eb4e47369c3fe8c62933a41a",
      "ee3c8ccadd56469f8701345be68d8b23",
      "3c5d5d384f3848399318e75bdbe78391",
      "f986f75a677b426dbb5648f57d6e7b2b",
      "5e73aa6aa00347acb71720caed4d2adb",
      "962760aad5ba44168c63011b15fc5e8a"
     ]
    },
    "id": "D0rH9vTup_fc",
    "outputId": "8adc3c13-9330-4fc4-c871-0c478cac9c7e"
   },
   "outputs": [],
   "source": [
    "# Labelled data\n",
    "\n",
    "labeled_texts = df_labeled['Text'].tolist()\n",
    "labeled_embeddings = model.encode(labeled_texts, show_progress_bar=True)\n",
    "embeddings_labeled = np.array(labeled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "TwW0K-jhsA56",
    "outputId": "f7e6e2d1-9ba2-40b9-a780-4bfcbb128934"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qN37d367uJKF"
   },
   "source": [
    "## Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B2jA1NAayB6W"
   },
   "outputs": [],
   "source": [
    "# Defining X and y\n",
    "\n",
    "X = embeddings_labeled             # sentence embeddings\n",
    "y = df_labeled['Label']            # labels\n",
    "text_data = df_labeled['Text']     # full texts for LLM justification\n",
    "\n",
    "X_train, X_test, y_train, y_test, text_train, text_test = train_test_split(\n",
    "    X, y, text_data,\n",
    "    test_size=0.2,\n",
    "    stratify=y,\n",
    "    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "Z__7tG3b_Uwq",
    "outputId": "f2c3ace6-fc0a-4141-cc72-acabe11fdeb2"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MHX-h6UH12yQ"
   },
   "source": [
    "## Cross-Validation on Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Dx0fwETFACh8"
   },
   "outputs": [],
   "source": [
    "# Defining various models to evaluate\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000, random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"Gradient Boosting\": GradientBoostingClassifier(random_state=42),\n",
    "    \"SVM (Linear Kernel)\": SVC(kernel='linear', probability=False, random_state=42),\n",
    "    \"Naive Bayes\": GaussianNB(),\n",
    "    \"XGBoost\": XGBClassifier(random_state=42, use_label_encoder=False, eval_metric='logloss')\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YI2Ez0yV3gGe",
    "outputId": "3ae05db1-d9d6-45f9-d160-368e8e8a5376"
   },
   "outputs": [],
   "source": [
    "y_train = np.array(y_train)\n",
    "\n",
    "all_metrics = []\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "for name, model in models.items():\n",
    "    acc_list, prec_list, rec_list, f1_list, f2_list = [], [], [], [], []\n",
    "\n",
    "    for train_idx, val_idx in cv.split(X_train, y_train):\n",
    "        X_tr, X_val = X_train[train_idx], X_train[val_idx]\n",
    "        y_tr, y_val = y_train[train_idx], y_train[val_idx]\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_val)\n",
    "\n",
    "        acc_list.append(accuracy_score(y_val, y_pred))\n",
    "        prec_list.append(precision_score(y_val, y_pred, zero_division=0))\n",
    "        rec_list.append(recall_score(y_val, y_pred))\n",
    "        f1_list.append(f1_score(y_val, y_pred))\n",
    "        f2_list.append(fbeta_score(y_val, y_pred, beta=2))\n",
    "\n",
    "    all_metrics.append([\n",
    "        name,\n",
    "        np.mean(acc_list),\n",
    "        np.mean(prec_list),\n",
    "        np.mean(rec_list),\n",
    "        np.mean(f1_list),\n",
    "        np.mean(f2_list)\n",
    "    ])\n",
    "\n",
    "metrics_df = pd.DataFrame(all_metrics, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1 Score\", \"F2 Score\"])\n",
    "metrics_df = metrics_df.sort_values(by=\"F2 Score\", ascending=False)\n",
    "\n",
    "print(\"Cross-Validation Metrics (Training Set):\")\n",
    "print(metrics_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLN6Qm34BbYI"
   },
   "source": [
    "Logistic Regression performs nearly as well as SVM (F1 = 0.85, F2 = 0.933) but trains much faster and is less computationally intensive.                          \n",
    "Therefore,choosing Logistic Regression is a more practical choice for final model due to its combination of strong performance and speed.                                                                          \n",
    "Also it provides interpretable outputs, which are valuable for explainability and integration with LLM-based justifications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yOUqHIYc2sl_",
    "outputId": "2a2510fd-9a2b-4cfd-8dc0-9b94a4621c1b"
   },
   "outputs": [],
   "source": [
    "# 1. Training on full training data\n",
    "final_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# 2. Predicting on test data\n",
    "y_pred = final_model.predict(X_test)\n",
    "y_proba = final_model.predict_proba(X_test)[:, 1]  # for ROC AUC\n",
    "\n",
    "# 3. Evaluating\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec = precision_score(y_test, y_pred)\n",
    "rec = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "f2 = fbeta_score(y_test, y_pred, beta=2)\n",
    "roc = roc_auc_score(y_test, y_proba)\n",
    "\n",
    "# 4. Printing Results\n",
    "print(f\"Test Set Metrics:\")\n",
    "print(f\"Accuracy: {acc:.4f}\")\n",
    "print(f\"Precision: {prec:.4f}\")\n",
    "print(f\"Recall: {rec:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"F2 Score: {f2:.4f}\")\n",
    "print(f\"ROC AUC: {roc:.4f}\")\n",
    "\n",
    "# 5. Confusion Matrix and Report\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "hTzFhDJFCZ52",
    "outputId": "d416d79e-f6cb-42ad-fcdf-7b3c4f035abb"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IHxVA_i64k7d"
   },
   "source": [
    "## LLM-Based Semantic Justification on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JCiGl9suChp2"
   },
   "outputs": [],
   "source": [
    "# Justification function\n",
    "def llm_based_justification(text):\n",
    "    if \"detect\" in text.lower():\n",
    "        return \"Paper seems to focus on detection, which is usually publishable due to practical impact.\"\n",
    "    elif \"safe\" in text.lower():\n",
    "        return \"Paper addresses safety, which is a core research concern and makes it suitable for publication.\"\n",
    "    elif \"model\" in text.lower():\n",
    "        return \"Paper proposes a model, indicating a structured contribution. Possibly publishable.\"\n",
    "    else:\n",
    "        return \"Paper lacks clarity or novelty in the abstract, which may limit publishability.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 112
    },
    "id": "ly4O2mwHChmd",
    "outputId": "6bb29200-e930-4789-8375-12d473a42ea5"
   },
   "outputs": [],
   "source": [
    "# Ensuring test variables are lists\n",
    "y_test_list = y_test.tolist() if hasattr(y_test, 'tolist') else list(y_test)\n",
    "text_test_list = list(text_test)  # ensure it's a list\n",
    "\n",
    "# Only evaluatng N samples\n",
    "N = min(10, len(text_test_list), len(y_test_list), len(y_pred))\n",
    "\n",
    "justifications = []\n",
    "\n",
    "for i in range(N):\n",
    "    text = text_test_list[i]\n",
    "    justification = llm_based_justification(text)\n",
    "\n",
    "    justifications.append({\n",
    "        \"Text\": text[:200] + \"...\",  # Truncate for readability\n",
    "        \"Actual Label\": y_test_list[i],\n",
    "        \"Model Prediction\": y_pred[i],\n",
    "        \"LLM Justification\": justification })\n",
    "\n",
    "llm_output_df = pd.DataFrame(justifications)\n",
    "llm_output_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "id": "S0jONKrHH7Nq",
    "outputId": "6fb6834a-b48f-4c83-c68d-c1cb35ea0ba1"
   },
   "outputs": [],
   "source": [
    "display(HTML(\"<hr>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v15NCj-bDz63"
   },
   "source": [
    "# Working on Unlabelled Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tH_zuHTOLEIT"
   },
   "source": [
    "## Saving both Predicted Label and LLM Justification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "4197180909254be3b5b0664c60c6ade2",
      "c2dbb886fda740eba36e4ad87fd5f25f",
      "eb98a2cf39f94f85be1723036b36fcd1",
      "c0bb4d1fec364b6998cf8219ad32e9f9",
      "317ef6d0f98d4cfcb13f68274aab7d0f",
      "ab5e0efcbd0d4af280e2934178922d5a",
      "37a1180fa7184ca89503230e705751fb",
      "e6862cbb00af4cfd8e913f9b8f380341",
      "e882ecbffcf948df977351aab29e5fe3",
      "d087d53dbf144a6882d9f6b1e39ad941",
      "c39c9d47f09347f88cd087f87d2ea702"
     ]
    },
    "id": "bmexf0HrChUK",
    "outputId": "70d6b5e4-ce5d-46ca-d7dd-fca0a15001c8"
   },
   "outputs": [],
   "source": [
    "# Generating Embeddings for Unlabelled Data\n",
    "\n",
    "df_unlabeled['embedding'] = df_unlabeled['Text'].apply(get_norm_embedding)\n",
    "\n",
    "unlabeled_texts = df_unlabeled['Text'].tolist()\n",
    "unlabeled_embeddings = model.encode(unlabeled_texts, show_progress_bar=True)\n",
    "embeddings_unlabeled = np.array(unlabeled_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NikFb7BuDvqg"
   },
   "outputs": [],
   "source": [
    "# Predicting Labels Using Final ML Model (Logistic Regression)\n",
    "\n",
    "unseen_preds = final_model.predict(embeddings_unlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "dU2mUz3THqNg",
    "outputId": "d6f52bba-c161-45fe-9942-4955de163716"
   },
   "outputs": [],
   "source": [
    "# Generating Justifications with LLM\n",
    "\n",
    "unseen_texts = df_unlabeled['Text'].tolist()\n",
    "\n",
    "unseen_justifications = []\n",
    "\n",
    "for i in range(len(unseen_preds)):\n",
    "    justification = llm_based_justification(unseen_texts[i])\n",
    "\n",
    "    unseen_justifications.append({\n",
    "        \"Text\": unseen_texts[i][:200] + \"...\",\n",
    "        \"Predicted Label\": unseen_preds[i],\n",
    "        \"LLM Justification\": justification\n",
    "    })\n",
    "\n",
    "unseen_df = pd.DataFrame(unseen_justifications)\n",
    "unseen_df.to_csv(\"final_unlabeled_predictions_and_justifications.csv\", index=False)\n",
    "unseen_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 17
    },
    "id": "ZEOM1syaLdY1",
    "outputId": "59bf5f36-c0b3-4ddc-d192-aed46793dd84"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "files.download(\"final_unlabeled_predictions_and_justifications.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E92jRyJbMD3d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2FrbqisKMD69"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
